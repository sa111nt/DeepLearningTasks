{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebvqJaNU9bkH"
      },
      "source": [
        "# Wprowadzenie do sieci neuronowych i uczenia maszynowego - Sieci Rekurencyjne\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Prowadzący:** Piotr Baryczkowski, Jakub Bednarek<br>\n",
        "**Kontakt:** piotr.baryczkowski@put.poznan.pl<br>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlq47LA0BuBB"
      },
      "source": [
        "## Cel ćwiczeń:\n",
        "- zapoznanie się z rekurencyjnymi sieciami neuronowymi,\n",
        "- stworzenie modelu sieci z warstwami rekurencyjnymi dla zbioru danych MNIST,\n",
        "- stworzenie własnych implementacji warstwami neuronowych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxLU8paIDmUe",
        "outputId": "eb6255b2-e7a4-4455-8249-50060fb3c79a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cpu\n",
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mzTzdZHPfEkv"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wV_u-YBWEJ8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7961ef7-924a-4f34-dbfb-f04f99f1ef90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 40.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.10MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.0MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.37MB/s]\n"
          ]
        }
      ],
      "source": [
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppmDSGoyFuJ9"
      },
      "source": [
        "## Sieci rekurencyjne\n",
        "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "\n",
        "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
        "\n",
        "Przykładowy model z warstwą rekurencyjną dla danych MNIST:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_YKDA5VkfEkw",
        "outputId": "1c6e2613-8357-4b62-a1ae-d65da69f6111",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel(\n",
              "  (lstm_1): LSTM(28, 128, batch_first=True)\n",
              "  (relu_1): ReLU()\n",
              "  (dense_1): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "class RecurrentModel(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(RecurrentModel, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        # Define your layers here.\n",
        "        self.lstm_1 = nn.LSTM(input_size=28, hidden_size=128, batch_first=True)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.dense_1 = nn.Linear(128, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        lstm_out, _ = self.lstm_1(inputs)\n",
        "        # Take the last output from the sequence (assume inputs are padded appropriately or have consistent lengths)\n",
        "        x = lstm_out[:, -1, :]  # Get the output of the last time step\n",
        "        x = self.relu_1(x)\n",
        "        x = self.dense_1(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "model = RecurrentModel(num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EgXZxT7SfEkw"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t-L7ZTjWfEkx",
        "outputId": "03de9d5d-cd1d-4211-cf0e-cde31c82e2fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302981  [   32/60000]\n",
            "loss: 1.883636  [ 3232/60000]\n",
            "loss: 1.828843  [ 6432/60000]\n",
            "loss: 1.951353  [ 9632/60000]\n",
            "loss: 1.706200  [12832/60000]\n",
            "loss: 1.715351  [16032/60000]\n",
            "loss: 1.831772  [19232/60000]\n",
            "loss: 1.771518  [22432/60000]\n",
            "loss: 1.741484  [25632/60000]\n",
            "loss: 1.672478  [28832/60000]\n",
            "loss: 1.650613  [32032/60000]\n",
            "loss: 1.790274  [35232/60000]\n",
            "loss: 1.618494  [38432/60000]\n",
            "loss: 1.680868  [41632/60000]\n",
            "loss: 1.610301  [44832/60000]\n",
            "loss: 1.614900  [48032/60000]\n",
            "loss: 1.546688  [51232/60000]\n",
            "loss: 1.582004  [54432/60000]\n",
            "loss: 1.501046  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 92.7%, Avg loss: 1.538454 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.593899  [   32/60000]\n",
            "loss: 1.523293  [ 3232/60000]\n",
            "loss: 1.530599  [ 6432/60000]\n",
            "loss: 1.515375  [ 9632/60000]\n",
            "loss: 1.543829  [12832/60000]\n",
            "loss: 1.695800  [16032/60000]\n",
            "loss: 1.555089  [19232/60000]\n",
            "loss: 1.495103  [22432/60000]\n",
            "loss: 1.618255  [25632/60000]\n",
            "loss: 1.496590  [28832/60000]\n",
            "loss: 1.478694  [32032/60000]\n",
            "loss: 1.562828  [35232/60000]\n",
            "loss: 1.582233  [38432/60000]\n",
            "loss: 1.513067  [41632/60000]\n",
            "loss: 1.474079  [44832/60000]\n",
            "loss: 1.465567  [48032/60000]\n",
            "loss: 1.577385  [51232/60000]\n",
            "loss: 1.497202  [54432/60000]\n",
            "loss: 1.528153  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.2%, Avg loss: 1.531628 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.556894  [   32/60000]\n",
            "loss: 1.554671  [ 3232/60000]\n",
            "loss: 1.554839  [ 6432/60000]\n",
            "loss: 1.645691  [ 9632/60000]\n",
            "loss: 1.492681  [12832/60000]\n",
            "loss: 1.511156  [16032/60000]\n",
            "loss: 1.497238  [19232/60000]\n",
            "loss: 1.483491  [22432/60000]\n",
            "loss: 1.465309  [25632/60000]\n",
            "loss: 1.548124  [28832/60000]\n",
            "loss: 1.518276  [32032/60000]\n",
            "loss: 1.489766  [35232/60000]\n",
            "loss: 1.507140  [38432/60000]\n",
            "loss: 1.555187  [41632/60000]\n",
            "loss: 1.578181  [44832/60000]\n",
            "loss: 1.472212  [48032/60000]\n",
            "loss: 1.492661  [51232/60000]\n",
            "loss: 1.471265  [54432/60000]\n",
            "loss: 1.523947  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.9%, Avg loss: 1.503361 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.503741  [   32/60000]\n",
            "loss: 1.496216  [ 3232/60000]\n",
            "loss: 1.461382  [ 6432/60000]\n",
            "loss: 1.522990  [ 9632/60000]\n",
            "loss: 1.461258  [12832/60000]\n",
            "loss: 1.523709  [16032/60000]\n",
            "loss: 1.495379  [19232/60000]\n",
            "loss: 1.528224  [22432/60000]\n",
            "loss: 1.528569  [25632/60000]\n",
            "loss: 1.523802  [28832/60000]\n",
            "loss: 1.531001  [32032/60000]\n",
            "loss: 1.492487  [35232/60000]\n",
            "loss: 1.516512  [38432/60000]\n",
            "loss: 1.525609  [41632/60000]\n",
            "loss: 1.465088  [44832/60000]\n",
            "loss: 1.495813  [48032/60000]\n",
            "loss: 1.491575  [51232/60000]\n",
            "loss: 1.493120  [54432/60000]\n",
            "loss: 1.491295  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.9%, Avg loss: 1.502777 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.529295  [   32/60000]\n",
            "loss: 1.465648  [ 3232/60000]\n",
            "loss: 1.461322  [ 6432/60000]\n",
            "loss: 1.467716  [ 9632/60000]\n",
            "loss: 1.516888  [12832/60000]\n",
            "loss: 1.491342  [16032/60000]\n",
            "loss: 1.491658  [19232/60000]\n",
            "loss: 1.500522  [22432/60000]\n",
            "loss: 1.461514  [25632/60000]\n",
            "loss: 1.488195  [28832/60000]\n",
            "loss: 1.492441  [32032/60000]\n",
            "loss: 1.514172  [35232/60000]\n",
            "loss: 1.497188  [38432/60000]\n",
            "loss: 1.530132  [41632/60000]\n",
            "loss: 1.492485  [44832/60000]\n",
            "loss: 1.462129  [48032/60000]\n",
            "loss: 1.461292  [51232/60000]\n",
            "loss: 1.495519  [54432/60000]\n",
            "loss: 1.518975  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.1%, Avg loss: 1.490556 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgtZzVYg1361"
      },
      "source": [
        "### Zadanie 1\n",
        "Rozszerz model z powyższego przykładu o kolejną warstwę rekurencyjną przed gęstą warstwą wyjściową.\n",
        "\n",
        "Standardowe sieci neuronowe generują jeden wynik na podstawie jednego inputu.\n",
        "Natomiast sieci rekurencyjne przetwarzają dane sekwencyjnie, w każdym kroku łącząc wynik poprzedniego przetwarzania i aktualnego wejścia. Dlatego domyślnym wejściem sieci neuronowej jest tensor 3-wymiarowy ([batch_size,sequence_size,sample_size]).\n",
        "Domyślnie warstwy rekurencyjne w PyTorchu zwracają sekwencje wyników wszystkich kroków przetwarzania dla warstwy rekurencyjnej. Jeśli chcesz zwrócić tylko wyniki ostatniego przetwarzania dla warstwy rekurencyjnej, musisz samemu to zaimplementować np. `x = lstm_out[:, -1, :]`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FzMsg5A7fEky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06fd60f-480c-49b4-f51f-c988bb360a0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel2(\n",
              "  (lstm_1): LSTM(28, 128, batch_first=True)\n",
              "  (lstm_2): LSTM(128, 128, batch_first=True)\n",
              "  (relu_1): ReLU()\n",
              "  (dense_1): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "class RecurrentModel2(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(RecurrentModel2, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.lstm_1 = nn.LSTM(input_size=28, hidden_size=128, batch_first=True)\n",
        "        self.lstm_2 = nn.LSTM(input_size=128, hidden_size=128, batch_first=True)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.dense_1 = nn.Linear(128, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            inputs = inputs.squeeze(1)\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        lstm_out_1, _ = self.lstm_1(inputs)\n",
        "        lstm_out_2, _ = self.lstm_2(lstm_out_1)\n",
        "        x = lstm_out_2[:, -1, :]\n",
        "        x = self.relu_1(x)\n",
        "        x = self.dense_1(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "model = RecurrentModel2(num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3ptuv6IHfEky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8136f47e-9779-42e7-a6f8-87d21f1ac3a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302454  [   32/60000]\n",
            "loss: 2.065291  [ 3232/60000]\n",
            "loss: 1.837247  [ 6432/60000]\n",
            "loss: 1.847266  [ 9632/60000]\n",
            "loss: 1.687922  [12832/60000]\n",
            "loss: 1.734693  [16032/60000]\n",
            "loss: 1.741557  [19232/60000]\n",
            "loss: 1.662582  [22432/60000]\n",
            "loss: 1.565365  [25632/60000]\n",
            "loss: 1.630697  [28832/60000]\n",
            "loss: 1.559054  [32032/60000]\n",
            "loss: 1.500766  [35232/60000]\n",
            "loss: 1.567310  [38432/60000]\n",
            "loss: 1.527837  [41632/60000]\n",
            "loss: 1.536370  [44832/60000]\n",
            "loss: 1.552933  [48032/60000]\n",
            "loss: 1.546464  [51232/60000]\n",
            "loss: 1.586277  [54432/60000]\n",
            "loss: 1.494086  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.1%, Avg loss: 1.531878 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.573966  [   32/60000]\n",
            "loss: 1.525413  [ 3232/60000]\n",
            "loss: 1.500720  [ 6432/60000]\n",
            "loss: 1.495611  [ 9632/60000]\n",
            "loss: 1.492251  [12832/60000]\n",
            "loss: 1.574720  [16032/60000]\n",
            "loss: 1.496001  [19232/60000]\n",
            "loss: 1.599877  [22432/60000]\n",
            "loss: 1.534971  [25632/60000]\n",
            "loss: 1.561566  [28832/60000]\n",
            "loss: 1.610592  [32032/60000]\n",
            "loss: 1.553092  [35232/60000]\n",
            "loss: 1.471319  [38432/60000]\n",
            "loss: 1.513327  [41632/60000]\n",
            "loss: 1.493244  [44832/60000]\n",
            "loss: 1.595831  [48032/60000]\n",
            "loss: 1.461466  [51232/60000]\n",
            "loss: 1.492448  [54432/60000]\n",
            "loss: 1.492732  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.4%, Avg loss: 1.518262 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.541692  [   32/60000]\n",
            "loss: 1.461808  [ 3232/60000]\n",
            "loss: 1.544261  [ 6432/60000]\n",
            "loss: 1.502204  [ 9632/60000]\n",
            "loss: 1.482848  [12832/60000]\n",
            "loss: 1.494769  [16032/60000]\n",
            "loss: 1.555836  [19232/60000]\n",
            "loss: 1.462054  [22432/60000]\n",
            "loss: 1.493335  [25632/60000]\n",
            "loss: 1.492554  [28832/60000]\n",
            "loss: 1.539303  [32032/60000]\n",
            "loss: 1.555895  [35232/60000]\n",
            "loss: 1.552787  [38432/60000]\n",
            "loss: 1.586136  [41632/60000]\n",
            "loss: 1.524004  [44832/60000]\n",
            "loss: 1.501229  [48032/60000]\n",
            "loss: 1.463737  [51232/60000]\n",
            "loss: 1.493392  [54432/60000]\n",
            "loss: 1.463080  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.3%, Avg loss: 1.498741 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.461287  [   32/60000]\n",
            "loss: 1.461263  [ 3232/60000]\n",
            "loss: 1.503590  [ 6432/60000]\n",
            "loss: 1.461897  [ 9632/60000]\n",
            "loss: 1.523191  [12832/60000]\n",
            "loss: 1.524111  [16032/60000]\n",
            "loss: 1.492231  [19232/60000]\n",
            "loss: 1.492746  [22432/60000]\n",
            "loss: 1.519125  [25632/60000]\n",
            "loss: 1.461291  [28832/60000]\n",
            "loss: 1.461226  [32032/60000]\n",
            "loss: 1.514616  [35232/60000]\n",
            "loss: 1.492440  [38432/60000]\n",
            "loss: 1.555760  [41632/60000]\n",
            "loss: 1.461802  [44832/60000]\n",
            "loss: 1.511091  [48032/60000]\n",
            "loss: 1.461179  [51232/60000]\n",
            "loss: 1.461180  [54432/60000]\n",
            "loss: 1.463125  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.2%, Avg loss: 1.499222 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.461294  [   32/60000]\n",
            "loss: 1.462542  [ 3232/60000]\n",
            "loss: 1.461169  [ 6432/60000]\n",
            "loss: 1.523506  [ 9632/60000]\n",
            "loss: 1.492501  [12832/60000]\n",
            "loss: 1.492597  [16032/60000]\n",
            "loss: 1.590097  [19232/60000]\n",
            "loss: 1.516923  [22432/60000]\n",
            "loss: 1.492441  [25632/60000]\n",
            "loss: 1.461189  [28832/60000]\n",
            "loss: 1.461721  [32032/60000]\n",
            "loss: 1.461171  [35232/60000]\n",
            "loss: 1.487887  [38432/60000]\n",
            "loss: 1.461423  [41632/60000]\n",
            "loss: 1.461197  [44832/60000]\n",
            "loss: 1.491548  [48032/60000]\n",
            "loss: 1.523660  [51232/60000]\n",
            "loss: 1.554757  [54432/60000]\n",
            "loss: 1.554751  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 1.486239 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYDLWjdseB4H"
      },
      "source": [
        "### Zadanie 2\n",
        "Wykorzystując model z przykładu, napisz sieć rekurencyjną przy użyciu RNNCell.\n",
        "\n",
        "RNNCell implementuje tylko operacje wykonywane przez warstwę\n",
        "rekurencyjną dla jednego kroku. Warstwy rekurencyjne w każdym kroku\n",
        "łączą wynik operacji poprzedniego kroku i aktualny input.\n",
        "Wykorzystaj pętle for do wielokrotnego wywołania komórki RNNCell (liczba kroków to liczba elementów w sekwencji).\n",
        "\n",
        "Wywołanie zainicjalizowanej komórki rekurencyjnej wymaga podania aktualnego inputu i listy stanów ukrytych poprzedniego kroku (RNNCell ma jeden stan).\n",
        "\n",
        "Trzeba zainicjalizować ukryty stan warstwy z wartościami początkowymi (można wykorzystać zmienne losowe - torch.rand)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UXwkHVXLfEky"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RecurrentModel3(nn.Module):\n",
        "    def __init__(self, input_size=28, hidden_size=128, num_classes=10):\n",
        "        super(RecurrentModel3, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Define the RNN cell\n",
        "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.dense_1 = nn.Linear(hidden_size, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        # Initialize hidden state\n",
        "        batch_size = inputs.size(0)\n",
        "        h = torch.zeros(batch_size, self.hidden_size, device=inputs.device)\n",
        "\n",
        "        # Process sequence step by step\n",
        "        for t in range(inputs.size(1)): # inputs.size(1) is the sequence_length (28 for MNIST images)\n",
        "            h = self.rnn_cell(inputs[:, t, :], h)\n",
        "\n",
        "        # Take the final hidden state and pass it through the dense layer\n",
        "        x = self.relu_1(h)\n",
        "        x = self.dense_1(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "model = RecurrentModel3(input_size=28, hidden_size=128, num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CVmL0U34fEky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fbe6429-b3fd-4aba-8459-32d57becd0c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302114  [   32/60000]\n",
            "loss: 2.295591  [ 3232/60000]\n",
            "loss: 2.246437  [ 6432/60000]\n",
            "loss: 2.224259  [ 9632/60000]\n",
            "loss: 2.081541  [12832/60000]\n",
            "loss: 2.100355  [16032/60000]\n",
            "loss: 2.110106  [19232/60000]\n",
            "loss: 1.948715  [22432/60000]\n",
            "loss: 1.833246  [25632/60000]\n",
            "loss: 2.134346  [28832/60000]\n",
            "loss: 1.883807  [32032/60000]\n",
            "loss: 2.270484  [35232/60000]\n",
            "loss: 2.054603  [38432/60000]\n",
            "loss: 1.976045  [41632/60000]\n",
            "loss: 2.001590  [44832/60000]\n",
            "loss: 1.839967  [48032/60000]\n",
            "loss: 1.868906  [51232/60000]\n",
            "loss: 1.782172  [54432/60000]\n",
            "loss: 1.806194  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.3%, Avg loss: 1.792229 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.859068  [   32/60000]\n",
            "loss: 1.782731  [ 3232/60000]\n",
            "loss: 1.802533  [ 6432/60000]\n",
            "loss: 1.722198  [ 9632/60000]\n",
            "loss: 1.681407  [12832/60000]\n",
            "loss: 1.770567  [16032/60000]\n",
            "loss: 1.688006  [19232/60000]\n",
            "loss: 1.826825  [22432/60000]\n",
            "loss: 1.753467  [25632/60000]\n",
            "loss: 1.623357  [28832/60000]\n",
            "loss: 1.627762  [32032/60000]\n",
            "loss: 1.806986  [35232/60000]\n",
            "loss: 1.739660  [38432/60000]\n",
            "loss: 1.656163  [41632/60000]\n",
            "loss: 1.752447  [44832/60000]\n",
            "loss: 1.695235  [48032/60000]\n",
            "loss: 1.834235  [51232/60000]\n",
            "loss: 1.491139  [54432/60000]\n",
            "loss: 1.608969  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.8%, Avg loss: 1.667376 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.641990  [   32/60000]\n",
            "loss: 1.711966  [ 3232/60000]\n",
            "loss: 1.747792  [ 6432/60000]\n",
            "loss: 1.600898  [ 9632/60000]\n",
            "loss: 1.588418  [12832/60000]\n",
            "loss: 1.547157  [16032/60000]\n",
            "loss: 1.684719  [19232/60000]\n",
            "loss: 1.660068  [22432/60000]\n",
            "loss: 1.582368  [25632/60000]\n",
            "loss: 1.646554  [28832/60000]\n",
            "loss: 1.649664  [32032/60000]\n",
            "loss: 1.673159  [35232/60000]\n",
            "loss: 1.887038  [38432/60000]\n",
            "loss: 1.592045  [41632/60000]\n",
            "loss: 1.708962  [44832/60000]\n",
            "loss: 1.615338  [48032/60000]\n",
            "loss: 1.614289  [51232/60000]\n",
            "loss: 1.664956  [54432/60000]\n",
            "loss: 1.724715  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.5%, Avg loss: 1.617789 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.633039  [   32/60000]\n",
            "loss: 1.558548  [ 3232/60000]\n",
            "loss: 1.666021  [ 6432/60000]\n",
            "loss: 1.553061  [ 9632/60000]\n",
            "loss: 1.618523  [12832/60000]\n",
            "loss: 1.583447  [16032/60000]\n",
            "loss: 1.536841  [19232/60000]\n",
            "loss: 1.747306  [22432/60000]\n",
            "loss: 1.588203  [25632/60000]\n",
            "loss: 2.181910  [28832/60000]\n",
            "loss: 1.556302  [32032/60000]\n",
            "loss: 1.675815  [35232/60000]\n",
            "loss: 1.522121  [38432/60000]\n",
            "loss: 1.624172  [41632/60000]\n",
            "loss: 1.602186  [44832/60000]\n",
            "loss: 1.652859  [48032/60000]\n",
            "loss: 1.589447  [51232/60000]\n",
            "loss: 1.674444  [54432/60000]\n",
            "loss: 1.561636  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.5%, Avg loss: 1.598931 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.641339  [   32/60000]\n",
            "loss: 1.692758  [ 3232/60000]\n",
            "loss: 1.535811  [ 6432/60000]\n",
            "loss: 1.800777  [ 9632/60000]\n",
            "loss: 1.523608  [12832/60000]\n",
            "loss: 1.852988  [16032/60000]\n",
            "loss: 1.582991  [19232/60000]\n",
            "loss: 1.677337  [22432/60000]\n",
            "loss: 1.615452  [25632/60000]\n",
            "loss: 1.555694  [28832/60000]\n",
            "loss: 1.690341  [32032/60000]\n",
            "loss: 1.523627  [35232/60000]\n",
            "loss: 1.623268  [38432/60000]\n",
            "loss: 1.465776  [41632/60000]\n",
            "loss: 1.556059  [44832/60000]\n",
            "loss: 1.509879  [48032/60000]\n",
            "loss: 1.602341  [51232/60000]\n",
            "loss: 1.556750  [54432/60000]\n",
            "loss: 1.585889  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.2%, Avg loss: 1.640651 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyPGkC6oiEd5"
      },
      "source": [
        "### Zadanie 3\n",
        "Zamień komórkę rekurencyjną z poprzedniego zadania na LSTMCell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "C5MPQ1UcigN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67fe37a8-95d9-4e3b-83a5-fbe276847a85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel4(\n",
              "  (lstm_cell): LSTMCell(28, 128)\n",
              "  (relu_1): ReLU()\n",
              "  (dense_1): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "class RecurrentModel4(nn.Module):\n",
        "    def __init__(self, input_size=28, hidden_size=128, num_classes=10):\n",
        "        super(RecurrentModel4, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Define the LSTM cell\n",
        "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.dense_1 = nn.Linear(hidden_size, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        # Initialize hidden state and cell state\n",
        "        batch_size = inputs.size(0)\n",
        "        h = torch.zeros(batch_size, self.hidden_size, device=inputs.device)\n",
        "        c = torch.zeros(batch_size, self.hidden_size, device=inputs.device)\n",
        "\n",
        "        # Process sequence step by step\n",
        "        for t in range(inputs.size(1)): # inputs.size(1) is the sequence_length (28 for MNIST images)\n",
        "            h, c = self.lstm_cell(inputs[:, t, :], (h, c))\n",
        "\n",
        "        # Take the final hidden state and pass it through the dense layer\n",
        "        x = self.relu_1(h)\n",
        "        x = self.dense_1(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "model = RecurrentModel4(input_size=28, hidden_size=128, num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8cxbdnUNfEkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9cdad1a-1e0f-4aa4-becb-a31d57815509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.304571  [   32/60000]\n",
            "loss: 2.056720  [ 3232/60000]\n",
            "loss: 1.933532  [ 6432/60000]\n",
            "loss: 1.788264  [ 9632/60000]\n",
            "loss: 1.676427  [12832/60000]\n",
            "loss: 1.695085  [16032/60000]\n",
            "loss: 1.613389  [19232/60000]\n",
            "loss: 1.759412  [22432/60000]\n",
            "loss: 1.732003  [25632/60000]\n",
            "loss: 1.615311  [28832/60000]\n",
            "loss: 1.718195  [32032/60000]\n",
            "loss: 1.611797  [35232/60000]\n",
            "loss: 1.585010  [38432/60000]\n",
            "loss: 1.603897  [41632/60000]\n",
            "loss: 1.497465  [44832/60000]\n",
            "loss: 1.526759  [48032/60000]\n",
            "loss: 1.527915  [51232/60000]\n",
            "loss: 1.568891  [54432/60000]\n",
            "loss: 1.504753  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 90.9%, Avg loss: 1.555202 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.580806  [   32/60000]\n",
            "loss: 1.519574  [ 3232/60000]\n",
            "loss: 1.529403  [ 6432/60000]\n",
            "loss: 1.517452  [ 9632/60000]\n",
            "loss: 1.650924  [12832/60000]\n",
            "loss: 1.537828  [16032/60000]\n",
            "loss: 1.577263  [19232/60000]\n",
            "loss: 1.610891  [22432/60000]\n",
            "loss: 1.558320  [25632/60000]\n",
            "loss: 1.495877  [28832/60000]\n",
            "loss: 1.514262  [32032/60000]\n",
            "loss: 1.517782  [35232/60000]\n",
            "loss: 1.491466  [38432/60000]\n",
            "loss: 1.566840  [41632/60000]\n",
            "loss: 1.555667  [44832/60000]\n",
            "loss: 1.523858  [48032/60000]\n",
            "loss: 1.514703  [51232/60000]\n",
            "loss: 1.588594  [54432/60000]\n",
            "loss: 1.528593  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 91.5%, Avg loss: 1.548542 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.539020  [   32/60000]\n",
            "loss: 1.515388  [ 3232/60000]\n",
            "loss: 1.561984  [ 6432/60000]\n",
            "loss: 1.524113  [ 9632/60000]\n",
            "loss: 1.564464  [12832/60000]\n",
            "loss: 1.501232  [16032/60000]\n",
            "loss: 1.523211  [19232/60000]\n",
            "loss: 1.524922  [22432/60000]\n",
            "loss: 1.500464  [25632/60000]\n",
            "loss: 1.464365  [28832/60000]\n",
            "loss: 1.470981  [32032/60000]\n",
            "loss: 1.556493  [35232/60000]\n",
            "loss: 1.492746  [38432/60000]\n",
            "loss: 1.492496  [41632/60000]\n",
            "loss: 1.486016  [44832/60000]\n",
            "loss: 1.461319  [48032/60000]\n",
            "loss: 1.496162  [51232/60000]\n",
            "loss: 1.527445  [54432/60000]\n",
            "loss: 1.559244  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.3%, Avg loss: 1.508528 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.523710  [   32/60000]\n",
            "loss: 1.496327  [ 3232/60000]\n",
            "loss: 1.527147  [ 6432/60000]\n",
            "loss: 1.524238  [ 9632/60000]\n",
            "loss: 1.592382  [12832/60000]\n",
            "loss: 1.568275  [16032/60000]\n",
            "loss: 1.511578  [19232/60000]\n",
            "loss: 1.492454  [22432/60000]\n",
            "loss: 1.524700  [25632/60000]\n",
            "loss: 1.523478  [28832/60000]\n",
            "loss: 1.493538  [32032/60000]\n",
            "loss: 1.521967  [35232/60000]\n",
            "loss: 1.516006  [38432/60000]\n",
            "loss: 1.543565  [41632/60000]\n",
            "loss: 1.475270  [44832/60000]\n",
            "loss: 1.470985  [48032/60000]\n",
            "loss: 1.544466  [51232/60000]\n",
            "loss: 1.492517  [54432/60000]\n",
            "loss: 1.491385  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 1.504441 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.536183  [   32/60000]\n",
            "loss: 1.461434  [ 3232/60000]\n",
            "loss: 1.461278  [ 6432/60000]\n",
            "loss: 1.461193  [ 9632/60000]\n",
            "loss: 1.520233  [12832/60000]\n",
            "loss: 1.480964  [16032/60000]\n",
            "loss: 1.492541  [19232/60000]\n",
            "loss: 1.492593  [22432/60000]\n",
            "loss: 1.492151  [25632/60000]\n",
            "loss: 1.491535  [28832/60000]\n",
            "loss: 1.499173  [32032/60000]\n",
            "loss: 1.503990  [35232/60000]\n",
            "loss: 1.461401  [38432/60000]\n",
            "loss: 1.462423  [41632/60000]\n",
            "loss: 1.461596  [44832/60000]\n",
            "loss: 1.523421  [48032/60000]\n",
            "loss: 1.538660  [51232/60000]\n",
            "loss: 1.493401  [54432/60000]\n",
            "loss: 1.497726  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.2%, Avg loss: 1.499668 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prwjaEv2efs3"
      },
      "source": [
        "### Zadanie 4\n",
        "Wykorzystując model z poprzedniego zadania, stwórz model sieci\n",
        "neuronowej z własną implementacją prostej warstwy rekurencyjnej.\n",
        "- w call zamień self.lstm_cell_layer(x) na wyołanie własnej metody np. self.cell(x)\n",
        "- w konstruktorze modelu usuń inicjalizację komórki LSTM i zastąp ją inicjalizacją warstw potrzebnych do stworzenia własnej komórki rekurencyjnej,\n",
        "- stwórz metodę cell() wykonującą operacje warstwy rekurencyjnej,\n",
        "- prosta warstwa rekurencyjna konkatenuje poprzedni wyniki i aktualny input, a następnie przepuszcza ten połączony tensor przez warstwę gęstą (Dense)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tn5PgdhHfEkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ee38fd-0b02-4cf1-d20d-cd3513f41a64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel5(\n",
              "  (linear_cell): Linear(in_features=156, out_features=128, bias=True)\n",
              "  (tanh): Tanh()\n",
              "  (relu_1): ReLU()\n",
              "  (dense_1): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "class RecurrentModel5(nn.Module):\n",
        "    def __init__(self, input_size=28, hidden_size=128, num_classes=10):\n",
        "        super(RecurrentModel5, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Define layers for custom recurrent cell\n",
        "        self.linear_cell = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        # Output layers\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.dense_1 = nn.Linear(hidden_size, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def cell(self, x, h):\n",
        "        combined_input = torch.cat((x, h), dim=1)\n",
        "        h_new = self.tanh(self.linear_cell(combined_input))\n",
        "        return h_new\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        # Initialize hidden state\n",
        "        batch_size = inputs.size(0)\n",
        "        h = torch.zeros(batch_size, self.hidden_size, device=inputs.device)\n",
        "\n",
        "        # Process sequence step by step using the custom cell\n",
        "        for t in range(inputs.size(1)): # inputs.size(1) is the sequence_length (28 for MNIST images)\n",
        "            h = self.cell(inputs[:, t, :], h)\n",
        "\n",
        "        # Take the final hidden state and pass it through the dense layer\n",
        "        x = self.relu_1(h)\n",
        "        x = self.dense_1(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "model = RecurrentModel5(input_size=28, hidden_size=128, num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uNNkN9LGfEkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df1a97b-7d6f-4b67-b408-44cb6f9bfcd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.300291  [   32/60000]\n",
            "loss: 2.293908  [ 3232/60000]\n",
            "loss: 2.165182  [ 6432/60000]\n",
            "loss: 2.146461  [ 9632/60000]\n",
            "loss: 2.102144  [12832/60000]\n",
            "loss: 2.053736  [16032/60000]\n",
            "loss: 2.040306  [19232/60000]\n",
            "loss: 2.043575  [22432/60000]\n",
            "loss: 2.037921  [25632/60000]\n",
            "loss: 1.978972  [28832/60000]\n",
            "loss: 1.907864  [32032/60000]\n",
            "loss: 1.825292  [35232/60000]\n",
            "loss: 1.703875  [38432/60000]\n",
            "loss: 2.061726  [41632/60000]\n",
            "loss: 1.848050  [44832/60000]\n",
            "loss: 1.850106  [48032/60000]\n",
            "loss: 1.948858  [51232/60000]\n",
            "loss: 1.984806  [54432/60000]\n",
            "loss: 1.978574  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.5%, Avg loss: 1.841141 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.850295  [   32/60000]\n",
            "loss: 1.972953  [ 3232/60000]\n",
            "loss: 2.249990  [ 6432/60000]\n",
            "loss: 1.839846  [ 9632/60000]\n",
            "loss: 2.079344  [12832/60000]\n",
            "loss: 1.841746  [16032/60000]\n",
            "loss: 1.636889  [19232/60000]\n",
            "loss: 1.846280  [22432/60000]\n",
            "loss: 1.869632  [25632/60000]\n",
            "loss: 2.095574  [28832/60000]\n",
            "loss: 1.964492  [32032/60000]\n",
            "loss: 2.008378  [35232/60000]\n",
            "loss: 1.914513  [38432/60000]\n",
            "loss: 1.785315  [41632/60000]\n",
            "loss: 1.772786  [44832/60000]\n",
            "loss: 1.746816  [48032/60000]\n",
            "loss: 1.856372  [51232/60000]\n",
            "loss: 1.773244  [54432/60000]\n",
            "loss: 1.813656  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 49.6%, Avg loss: 1.964238 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.881753  [   32/60000]\n",
            "loss: 1.776689  [ 3232/60000]\n",
            "loss: 1.819322  [ 6432/60000]\n",
            "loss: 1.818309  [ 9632/60000]\n",
            "loss: 1.826592  [12832/60000]\n",
            "loss: 1.932426  [16032/60000]\n",
            "loss: 1.951067  [19232/60000]\n",
            "loss: 1.791097  [22432/60000]\n",
            "loss: 1.838224  [25632/60000]\n",
            "loss: 1.652136  [28832/60000]\n",
            "loss: 1.729831  [32032/60000]\n",
            "loss: 1.697135  [35232/60000]\n",
            "loss: 1.808149  [38432/60000]\n",
            "loss: 1.699986  [41632/60000]\n",
            "loss: 1.776467  [44832/60000]\n",
            "loss: 1.719780  [48032/60000]\n",
            "loss: 1.964220  [51232/60000]\n",
            "loss: 1.978764  [54432/60000]\n",
            "loss: 1.897410  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.1%, Avg loss: 1.860531 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.773417  [   32/60000]\n",
            "loss: 1.873291  [ 3232/60000]\n",
            "loss: 1.666013  [ 6432/60000]\n",
            "loss: 1.900805  [ 9632/60000]\n",
            "loss: 1.841046  [12832/60000]\n",
            "loss: 1.836349  [16032/60000]\n",
            "loss: 1.901709  [19232/60000]\n",
            "loss: 1.898646  [22432/60000]\n",
            "loss: 1.716521  [25632/60000]\n",
            "loss: 1.818396  [28832/60000]\n",
            "loss: 1.856408  [32032/60000]\n",
            "loss: 1.843943  [35232/60000]\n",
            "loss: 1.934730  [38432/60000]\n",
            "loss: 1.727338  [41632/60000]\n",
            "loss: 1.867113  [44832/60000]\n",
            "loss: 1.617721  [48032/60000]\n",
            "loss: 1.829887  [51232/60000]\n",
            "loss: 1.717684  [54432/60000]\n",
            "loss: 1.802179  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 28.2%, Avg loss: 2.168489 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.136069  [   32/60000]\n",
            "loss: 1.688146  [ 3232/60000]\n",
            "loss: 1.872716  [ 6432/60000]\n",
            "loss: 2.001851  [ 9632/60000]\n",
            "loss: 1.778620  [12832/60000]\n",
            "loss: 1.747552  [16032/60000]\n",
            "loss: 1.810988  [19232/60000]\n",
            "loss: 2.248570  [22432/60000]\n",
            "loss: 2.034048  [25632/60000]\n",
            "loss: 1.903454  [28832/60000]\n",
            "loss: 2.066085  [32032/60000]\n",
            "loss: 1.800728  [35232/60000]\n",
            "loss: 1.759332  [38432/60000]\n",
            "loss: 1.775360  [41632/60000]\n",
            "loss: 1.989004  [44832/60000]\n",
            "loss: 1.801235  [48032/60000]\n",
            "loss: 1.888811  [51232/60000]\n",
            "loss: 1.876806  [54432/60000]\n",
            "loss: 1.887914  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 25.7%, Avg loss: 2.201211 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3sOaUu3b77l"
      },
      "source": [
        "### Zadanie 5\n",
        "\n",
        "Na podstawie modelu z poprzedniego zadania stwórz model z własną implementacją warstwy LSTM. Dokładny i zrozumiały opis działania wartswy LSTM znajduje się na [stronie](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hkCPXSXnfEk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d56f468-e47a-49d9-91e9-61b5c6c47e43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RecurrentModel6(\n",
              "  (linear_f): Linear(in_features=156, out_features=128, bias=True)\n",
              "  (linear_i): Linear(in_features=156, out_features=128, bias=True)\n",
              "  (linear_c): Linear(in_features=156, out_features=128, bias=True)\n",
              "  (linear_o): Linear(in_features=156, out_features=128, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              "  (tanh): Tanh()\n",
              "  (relu_1): ReLU()\n",
              "  (dense_1): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from torch.nn.modules.activation import Sigmoid, Tanh\n",
        "\n",
        "class RecurrentModel6(nn.Module):\n",
        "    def __init__(self, input_size=28, hidden_size=128, num_classes=10):\n",
        "        super(RecurrentModel6, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Define LSTM layers (gates and candidate cell state)\n",
        "        self.linear_f = nn.Linear(input_size + hidden_size, hidden_size) # Forget gate\n",
        "        self.linear_i = nn.Linear(input_size + hidden_size, hidden_size) # Input gate\n",
        "        self.linear_c = nn.Linear(input_size + hidden_size, hidden_size) # Cell state candidate\n",
        "        self.linear_o = nn.Linear(input_size + hidden_size, hidden_size) # Output gate\n",
        "\n",
        "        self.sigmoid = Sigmoid()\n",
        "        self.tanh = Tanh()\n",
        "\n",
        "        # Output layers\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.dense_1 = nn.Linear(hidden_size, num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def cell(self, x, h_c_prev):\n",
        "        h_prev, c_prev = h_c_prev # Unpack previous hidden and cell states\n",
        "\n",
        "        combined = torch.cat((x, h_prev), dim=1)\n",
        "\n",
        "        # Forget Gate\n",
        "        f_t = self.sigmoid(self.linear_f(combined))\n",
        "        # Input Gate\n",
        "        i_t = self.sigmoid(self.linear_i(combined))\n",
        "        # Cell State Candidate\n",
        "        c_tilde_t = self.tanh(self.linear_c(combined))\n",
        "        # Output Gate\n",
        "        o_t = self.sigmoid(self.linear_o(combined))\n",
        "\n",
        "        # New Cell State\n",
        "        c_t = f_t * c_prev + i_t * c_tilde_t\n",
        "        # New Hidden State\n",
        "        h_t = o_t * self.tanh(c_t)\n",
        "\n",
        "        return h_t, c_t\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if inputs.dim() == 4:\n",
        "            # Example: (batch_size, channels, sequence_length, features)\n",
        "            inputs = inputs.squeeze(1)  # Remove the channels dimension if it's 1\n",
        "        elif inputs.dim() != 3:\n",
        "            raise ValueError(f\"Expected input to be 3D, got {inputs.dim()}D instead.\")\n",
        "\n",
        "        # Initialize hidden state and cell state\n",
        "        batch_size = inputs.size(0)\n",
        "        h = torch.zeros(batch_size, self.hidden_size, device=inputs.device)\n",
        "        c = torch.zeros(batch_size, self.hidden_size, device=inputs.device)\n",
        "\n",
        "        # Process sequence step by step using the custom cell\n",
        "        for t in range(inputs.size(1)): # inputs.size(1) is the sequence_length (28 for MNIST images)\n",
        "            h, c = self.cell(inputs[:, t, :], (h, c))\n",
        "\n",
        "        # Take the final hidden state and pass it through the dense layer\n",
        "        x = self.relu_1(h)\n",
        "        x = self.dense_1(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "model = RecurrentModel6(input_size=28, hidden_size=128, num_classes=10)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BQEm8GHqfEk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25dc3fc5-5f7d-4706-8e6e-ede18507ea11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.303797  [   32/60000]\n",
            "loss: 1.926665  [ 3232/60000]\n",
            "loss: 1.849037  [ 6432/60000]\n",
            "loss: 1.737481  [ 9632/60000]\n",
            "loss: 1.742795  [12832/60000]\n",
            "loss: 1.666287  [16032/60000]\n",
            "loss: 1.788786  [19232/60000]\n",
            "loss: 1.739735  [22432/60000]\n",
            "loss: 1.662443  [25632/60000]\n",
            "loss: 1.492035  [28832/60000]\n",
            "loss: 1.679692  [32032/60000]\n",
            "loss: 1.599578  [35232/60000]\n",
            "loss: 1.622489  [38432/60000]\n",
            "loss: 1.587174  [41632/60000]\n",
            "loss: 1.583508  [44832/60000]\n",
            "loss: 1.574126  [48032/60000]\n",
            "loss: 1.599679  [51232/60000]\n",
            "loss: 1.508448  [54432/60000]\n",
            "loss: 1.617748  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 91.3%, Avg loss: 1.550356 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.578380  [   32/60000]\n",
            "loss: 1.557972  [ 3232/60000]\n",
            "loss: 1.526417  [ 6432/60000]\n",
            "loss: 1.473073  [ 9632/60000]\n",
            "loss: 1.587164  [12832/60000]\n",
            "loss: 1.550855  [16032/60000]\n",
            "loss: 1.634084  [19232/60000]\n",
            "loss: 1.617076  [22432/60000]\n",
            "loss: 1.557010  [25632/60000]\n",
            "loss: 1.504239  [28832/60000]\n",
            "loss: 1.502175  [32032/60000]\n",
            "loss: 1.515700  [35232/60000]\n",
            "loss: 1.487635  [38432/60000]\n",
            "loss: 1.524729  [41632/60000]\n",
            "loss: 1.515928  [44832/60000]\n",
            "loss: 1.509517  [48032/60000]\n",
            "loss: 1.576294  [51232/60000]\n",
            "loss: 1.676991  [54432/60000]\n",
            "loss: 1.558588  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 91.9%, Avg loss: 1.541499 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 2\n",
        "learning_rate = 0.001\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff3dbaf8"
      },
      "source": [
        "# Task\n",
        "Zaimplementuj klasę `RecurrentModel2` w komórce `FzMsg5A7fEky`, dodając drugą warstwę rekurencyjną `nn.LSTM` po pierwszej warstwie LSTM, a przed warstwą ReLU i gęstą warstwą wyjściową, a następnie zaimplementuj metodę `forward` uwzględniającą tę nową warstwę. Następnie uruchom trening i testowanie zaktualizowanego modelu przez 5 epok, używając `CrossEntropyLoss` i `RMSprop`, uruchamiając komórkę `3ptuv6IHfEky`, a na koniec podsumuj uzyskane wyniki."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0a8de33"
      },
      "source": [
        "## Implement RecurrentModel2\n",
        "\n",
        "### Subtask:\n",
        "Uzupełnij klasę `RecurrentModel2` w komórce `FzMsg5A7fEky`. Dodaj drugą warstwę rekurencyjną `nn.LSTM` po pierwszej warstwie LSTM, a przed warstwą ReLU i gęstą warstwą wyjściową. Następnie zaimplementuj metodę `forward`, aby uwzględnić nową warstwę.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7d019c7"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing the `RecurrentModel2` class by adding a second LSTM layer and adjusting the `forward` method. The current code block for `RecurrentModel2` is incomplete. I will provide the complete code for `RecurrentModel2` within the specified cell.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv-snum",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}